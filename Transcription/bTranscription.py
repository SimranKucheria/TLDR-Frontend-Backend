import logging
import sys
import requests
import time
from helper import dirgetcheck
import swagger_client as cris_client
from config import config 
#from azure.storage.blob.baseblob import BaseBlobService
from azure.storage.blob  import BlobServiceClient, BlobBlock
from azure.storage.blob import generate_blob_sas, BlobSasPermissions
import os
#from azure.storage.blob.models import BlobPermissions
from datetime import datetime, timedelta
import uuid
from azure.core.exceptions import ResourceNotFoundError
from schemas.summary import transEntity
from Transcription.process_transcript import readj
from helper import dirgetcheck

SUBSCRIPTION_KEY = config.api_key
SERVICE_REGION = "centralindia"
DESCRIPTION = "Lecture Video"
LOCALE = "en-US"
container_name = 'forlecture'
account_name = config.storage_name
account_key = config.storage_key



def uploadtoaz(db,blob_name,dir):
    print(blob_name)
    print(os.path.join(dir,blob_name))   
    block_list=[]
    chunk_size=8192 
    blob_service_client = BlobServiceClient.from_connection_string(config.connect_str)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)

    try: 
        blob_client.get_blob_properties()
        print("File exists")

    except ResourceNotFoundError:
        print("\nUploading to Azure Storage as blob:\n\t" + blob_name)
        with open(os.path.join(dir,blob_name), "rb") as data:
            while True:
                read_data = data.read(chunk_size)
                if not read_data:
                    break
                blk_id = str(uuid.uuid4())
                blob_client.stage_block(block_id=blk_id,data=read_data) 
                block_list.append(BlobBlock(block_id=blk_id))
        blob_client.commit_block_list(block_list)
   
    sas_blob = generate_blob_sas(account_name=account_name, 
                                container_name=container_name,
                                blob_name=blob_name,
                                account_key=account_key,
                                permission=BlobSasPermissions(read=True),
                                expiry=datetime.utcnow() + timedelta(hours=1))
    url_with_sas = 'https://'+account_name+'.blob.core.windows.net/'+container_name+'/'+blob_name+'?'+sas_blob
    results = transcribe(url_with_sas,blob_name,db)
    return results

def transcribe_from_single_blob(uri, properties,blob_name):
    transcription_definition = cris_client.Transcription(
        display_name=blob_name[:-3],
        description=DESCRIPTION,
        locale=LOCALE,
        content_urls=[uri],
        properties=properties
    )

    return transcription_definition

def _paginate(api, paginated_object):
    """
    The autogenerated client does not support pagination. This function returns a generator over
    all items of the array that the paginated object `paginated_object` is part of.
    """
    yield from paginated_object.values
    typename = type(paginated_object).__name__
    auth_settings = ["apiKeyHeader", "apiKeyQuery"]
    while paginated_object.next_link:
        link = paginated_object.next_link[len(
            api.api_client.configuration.host):]
        paginated_object, status, headers = api.api_client.call_api(link, "GET",
                                                                    response_type=typename, auth_settings=auth_settings)

        if status == 200:
            yield from paginated_object.values
        else:
            raise Exception(
                f"could not receive paginated data: status {status}")

def transcribe(url_with_sas,blob_name,db):
    logging.info("Starting transcription client...")

        # configure API key authorization: subscription_key
    configuration = cris_client.Configuration()
    configuration.api_key["Ocp-Apim-Subscription-Key"] = SUBSCRIPTION_KEY
    configuration.host = f"https://{SERVICE_REGION}.api.cognitive.microsoft.com/speechtotext/v3.0"

        # create the client object and authenticate
    client = cris_client.ApiClient(configuration)

        # create an instance of the transcription api class
    api = cris_client.DefaultApi(api_client=client)
    if(transcription_id := db.trans.find_one({"blob_name": blob_name}) is None):
        # Specify transcription properties by passing a dict to the properties parameter. See
        # https://docs.microsoft.com/azure/cognitive-services/speech-service/batch-transcription#configuration-properties
        # for supported parameters.
        #container_sas_uri="https://btspeechtotext.blob.core.windows.net/forlecture?sp=rwl&st=2022-01-13T12:19:12Z&se=2022-01-13T20:19:12Z&spr=https&sv=2020-08-04&sr=c&sig=1Rq5NVkqbemky12HeWaIHNpSr9kWb%2F8X7bgCUzjxn%2FM%3D"
        properties = {
            "wordLevelTimestampsEnabled": True,
            "diarizationEnabled": True,
            "destinationContainerUrl": config.container_sas_uri,
            #"timeToLive": "PT1H"
        }
        
        transcription_definition = transcribe_from_single_blob(url_with_sas, properties, blob_name)

        created_transcription, status, headers = api.create_transcription_with_http_info(
            transcription=transcription_definition)

        # get the transcription Id from the location URI
        transcription_id = headers["location"].split("/")[-1]

        # Log information about the created transcription. If you should ask for support, please
        # include this information.
        logging.info(
            f"Created new transcription with id '{transcription_id}' in region {SERVICE_REGION}")

        logging.info("Checking status.")

        completed = False

        while not completed:
        # wait for 5 seconds before refreshing the transcription status
            time.sleep(5)

            transcription = api.get_transcription(transcription_id)
            logging.info(f"Transcriptions status: {transcription.status}")

            if transcription.status in ("Failed", "Succeeded"):
                completed = True
                item={'transcription_id':transcription_id,'blob_name':blob_name}
                db.trans.insert_one(transEntity(item))
    else:
        transcription_id = db.trans.find_one({"blob_name": blob_name})
        transcription_id = transcription_id['transcription_id']
    transcription = api.get_transcription(transcription_id)
    if transcription.status == "Succeeded":
        print("succeeded")
        pag_files = api.get_transcription_files(transcription_id)
        for file_data in _paginate(api, pag_files):
            if file_data.kind != "Transcription":
                continue
            global container_name
            results_url = file_data.links.content_url.split(container_name)
            print(results_url)
            blob_service_client = BlobServiceClient.from_connection_string(config.connect_str)
            blob_client = blob_service_client.get_blob_client(container=container_name, blob=results_url[1][1:])
            fname = transcription_id+'result.json'
            dir = dirgetcheck('Data','trans')
            with open(os.path.join(dir,fname),'wb') as dw:
                dw.write(blob_client.download_blob().readall())
            results = readj(os.path.join(dir,fname))
            print(results)
            return results
    elif transcription.status == "Failed":
        print("failed")
        logging.info(f"Transcription failed: {transcription.properties.error.message}")
        results = 'Failed'
        return results
        
    


